import feedparser
import pandas as pd
import datetime
import time
import concurrent.futures 
import sys
import calendar
from tqdm import tqdm 

# --- AI å’Œç½‘é¡µæŠ“å–åº“å·²å…¨éƒ¨ç§»é™¤ ---

# ====== é…ç½®åŒºåŸŸ ======
# --- AI API Key å·²ç§»é™¤ ---

RSS_FEEDS = {
    # ok
    # "CoinDesk": "",
    # 'c1':'https://www.coindesk.com/arc/outboundfeeds/rss',
    # 'c2':'https://cointelegraph.com/rss/category/op-ed',
    # 'c3':'https://cointelegraph.com/rss/category/hodlers-digest',
    # 'c4':'https://cointelegraph.com/rss/category/markets',
    # 'c5':'https://cointelegraph.com/rss',
    # 'c6':'https://thedefiant.io/feed/',
    # 'c7':'https://cryptonews.com/news/feed',
    # 'c9':'https://coinjournal.net/news/feed',
    # "Decrypt": "https://decrypt.co/feed",
    # "CryptoSlate": "https://cryptoslate.com/feed/",
    # "NewsBTC": "https://www.newsbtc.com/feed/",
    # 2
    # "Bloomberg_Crypto": "https://feeds.bloomberg.com/crypto/news.rss",
    # "Glassnode_Insights": "https://glassnode.substack.com/feed",

    # "Reuters_Finance": "https://www.reuters.com/markets/finance/rss/",
    # "CryptoQuant_Blog": "https://cryptoquant.com/feed",
    # "Dune_Blog": "https://dune.com/blog/rss.xml",
    # "Messari_All": "https://messari.io/rss/all.xml",
    # "Delphi_Digital": "https://members.delphidigital.io/feed",
    # "a16z_Crypto_Blog": "https://a16zcrypto.com/feed/",
    # "Paradigm_Blog": "https://www.paradigm.xyz/rss",
    # "Coindesk_Korea": "https://www.coindeskkorea.com/rss/allArticle.xml"
    # ok
    "Decrypt": "https://decrypt.co/feed",
    "CryptoSlate": "https://cryptoslate.com/feed/",
    "NewsBTC": "https://www.newsbtc.com/feed/",
    "Bloomberg_Crypto": "https://feeds.bloomberg.com/crypto/news.rss",
    "Glassnode_Insights": "https://glassnode.substack.com/feed",


# https://feeds.bloomberg.com/crypto/news.rss


}

TIME_WINDOW_START_UTC = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=24)
OUTPUT_FILE = "crypto_news_TEST.csv" # ä½¿ç”¨ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶å
MAX_WORKERS = 20 # è¿™ä¸ªè¿‡ç¨‹å¾ˆå¿«ï¼Œå¹¶å‘é«˜ä¸€ç‚¹æ²¡å…³ç³»


# ====== ä¿®å¤ Coindesk é“¾æ¥ (ä¿ç•™) ======
def fix_coindesk_url(url: str) -> str:
    if "coindesk.com" not in url: return url
    try:
        fixed = url.replace(",", "/")
        if not fixed.startswith("https://"): fixed = "https://" + fixed.lstrip("/")
        return fixed
    except Exception: return url

# --- extract_text_from_url (å·²ç§»é™¤) ---
# --- summarize_text_... (å·²ç§»é™¤) ---


# ====== â˜… ä¿®æ”¹ç‚¹ 1ï¼šå¹¶å‘å¤„ç†ï¼ˆæ— AI, æ— æŠ“å–ï¼‰ ======
def process_article(entry, source):
    """
    åªè¿›è¡Œæ—¥æœŸè¿‡æ»¤ï¼Œä¸æŠ“å–ç½‘é¡µï¼Œä¸è°ƒç”¨AI
    """
    title = entry.get('title', 'No Title Provided')
    
    # --- æ—¥æœŸè¿‡æ»¤ (ä¿ç•™) ---
    article_time_utc = None
    try:
        published_time_struct = entry.get("published_parsed")
        if not published_time_struct:
            tqdm.write(f"ğŸŸ¡ æ—¥æœŸç¼ºå¤± (è·³è¿‡): {title}")
            return None
        article_timestamp_utc = calendar.timegm(published_time_struct)
        article_time_utc = datetime.datetime.fromtimestamp(article_timestamp_utc, tz=datetime.timezone.utc)
    except Exception as e:
        tqdm.write(f"ğŸŸ¡ æ—¥æœŸè§£æå¤±è´¥: {entry.get('published', '')} - {e} (è·³è¿‡)")
        return None
    if article_time_utc < TIME_WINDOW_START_UTC:
        return None 
    # --- æ—¥æœŸè¿‡æ»¤ç»“æŸ ---

    link = fix_coindesk_url(entry.link)
    date_str = entry.get("published", "") # è¿™æ˜¯ Published å­—æ®µ
    
    # â˜… æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥è¿”å›æ•°æ®ï¼Œä¸æŠ“å–ä¹Ÿä¸æ€»ç»“
    return {
        "Title": title,
        "Summary": "", # â˜… ç›®æ ‡2: æ— AIï¼Œç•™ç©º
        "Link": link,
        "Published": date_str,
        "Source": source,
        "Date Batch": article_time_utc.strftime("%Yå¹´%mæœˆ%dæ—¥"), 
        "Description": "", # â˜… ç•™ç©º
    }


# ====== â˜… ä¿®æ”¹ç‚¹ 2ï¼šä¸»å‡½æ•°ï¼ˆæ›´æ–°æ—¥å¿—ï¼‰ ======
def fetch_rss_news():
    all_items = []
    tasks_to_process = []
    print(f"\nğŸ• å¼€å§‹æ”¶é›† RSS æºï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â„¹ï¸ ä»…ä¿ç•™ {TIME_WINDOW_START_UTC.strftime('%Y-%m-%d %H:%M:%S')} (UTC) ä¹‹åçš„æ–°é—»")
    print("â„¹ï¸ AI æ‘˜è¦æ¨¡å¼ (å·²å…³é—­)ã€‚") # â˜… ä¿®æ”¹æ—¥å¿—

    # é˜¶æ®µ 1ï¼šæ”¶é›† (â˜… ç›®æ ‡1: éªŒè¯RSSæº)
    rss_progress_bar = tqdm(RSS_FEEDS.items(), desc="ğŸ“¡ 1. æ‰«æRSSæº", unit="æº", leave=False)
    for source, url in rss_progress_bar:
        rss_progress_bar.set_description(f"ğŸ“¡ 1. æ‰«æä¸­: {source}")
        try:
            feed = feedparser.parse(url)
            # æ£€æŸ¥ feed.bozo å¯ä»¥åœ¨è§£æåç«‹åˆ»çŸ¥é“RSSæ˜¯å¦æ ¼å¼é”™è¯¯
            if feed.bozo:
                print(f"âŒ æŠ“å–è­¦å‘Šï¼š{source} (RSSæ ¼å¼å¯èƒ½ä¸è§„èŒƒ) - {feed.bozo_exception}")
                
            for entry in feed.entries:
                tasks_to_process.append((entry, source))
        except Exception as e:
            # è¿™é‡Œçš„æ—¥å¿—ä¼šæ•è· 404, 500, DNSé”™è¯¯ç­‰
            print(f"âŒ æŠ“å–RSSæºå¤±è´¥ï¼š{source} - {e}")
            
    print(f"\nâ„¹ï¸ æ”¶é›†åˆ° {len(tasks_to_process)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹å¹¶å‘å¤„ç†...")

    # é˜¶æ®µ 2ï¼šå¹¶å‘å¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_article, entry, source) for entry, source in tasks_to_process]
        
        # è¿™ä¸ªè¿›åº¦æ¡ä¼šè·‘å¾—éå¸¸å¿«
        ai_progress_bar = tqdm(
            concurrent.futures.as_completed(futures),
            total=len(tasks_to_process),
            desc="ğŸ“° 2. RSS æ¡ç›®å¤„ç†ä¸­", # â˜… ä¿®æ”¹æ—¥å¿—
            unit="ç¯‡"
        )
        
        for future in ai_progress_bar:
            result = future.result()
            if result:
                all_items.append(result)

    # é˜¶æ®µ 3ï¼šä¿å­˜ç»“æœ (â˜… ç›®æ ‡3: æ­£å¸¸å¯¼å‡ºCSV)
    if all_items:
        df = pd.DataFrame(all_items)
        
        # æŒ‰ç…§ä½ æˆªå›¾çš„é¡ºåºå®šä¹‰æœ€ç»ˆçš„åˆ—
        final_columns = [
            "Title", "Date Batch", "Description", "Link", 
            "Published", "Source", "Summary"
        ]
        
        # è¿‡æ»¤æ‰æ•°æ®ä¸­æ²¡æœ‰çš„åˆ—ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰ï¼Œå¹¶æ’åº
        present_columns = [col for col in final_columns if col in df.columns]
        df = df[present_columns] 
        
        df.drop_duplicates(subset=["Link"], inplace=True)
        
        try:
            df['parsed_date'] = pd.to_datetime(df['Published'], errors='coerce')
            df = df.sort_values(by='parsed_date', ascending=False).drop(columns=['parsed_date'])
        except Exception:
            print("âš ï¸ æ—¥æœŸæ ¼å¼ä¸ç»Ÿä¸€ï¼Œæœªè¿›è¡Œæ’åºã€‚")
            
        df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å·²ä¿å­˜ {len(df)} æ¡æ–°é—»æ¡ç›®åˆ° {OUTPUT_FILE}")
    else:
        print(f"âš ï¸ æœªè·å–åˆ°è¿‡å»24å°æ—¶å†…çš„æ–°é—»ã€‚")


# ====== æ‰‹åŠ¨æ‰§è¡Œå…¥å£ ======
if __name__ == "__main__":
    start = time.time()
    fetch_rss_news()
    print(f"\nâ±ï¸ æ€»è€—æ—¶ï¼š{round(time.time() - start, 2)} ç§’")

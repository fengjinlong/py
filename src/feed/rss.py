import feedparser
import pandas as pd
import datetime
import requests
import newspaper  # â˜… 1. å¯¼å…¥ newspaper3k
from time import time
import concurrent.futures
import sys
import calendar
from zhipuai import ZhipuAI  # å¯¼å…¥ æ™ºè°±AI
from tqdm import tqdm

# (newspaper3k ä¼šè‡ªåŠ¨å¤„ç† lxmlï¼Œè¿™é‡Œä¸å†éœ€è¦)

# ====== é…ç½®åŒºåŸŸ ======
# ï¼ï¼ï¼æ³¨æ„ï¼šè¯·åŠ¡å¿…æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ KEYï¼ï¼ï¼
# ä» æ™ºè°±AI å¼€æ”¾å¹³å° (open.bigmodel.cn) è·å–
ZHIPU_API_KEY = "06314b501e4a4135b1989d56c32a2324.495r1QkUVZQmWD0W"  # â†â†â† æ›¿æ¢æˆä½ çš„ Key
if "YOUR_ZHIPU_API_KEY_HERE" in ZHIPU_API_KEY:
    print("é”™è¯¯ï¼šè¯·åœ¨è„šæœ¬ä¸­è®¾ç½®ä½ çš„ ZHIPU_API_KEYã€‚", file=sys.stderr)
    sys.exit(1)
    
# â˜… é…ç½® æ™ºè°±AI å®¢æˆ·ç«¯
client = ZhipuAI(api_key=ZHIPU_API_KEY)

RSS_FEEDS = {
    'c1': 'https://www.coindesk.com/arc/outboundfeeds/rss',
    'c2': 'https://cointelegraph.com/rss/category/op-ed',
    'c3': 'https://cointelegraph.com/rss/category/hodlers-digest',
    'c4': 'https://cointelegraph.com/rss/category/markets',
    'c6': 'https://thedefiant.io/feed/',
    'c7': 'https://cryptonews.com/news/feed',
    'c9': 'https://coinjournal.net/news/feed',
    "Decrypt": "https://decrypt.co/feed",
    # "CryptoSlate": "https://cryptoslate.com/feed/", rss
    "NewsBTC": "https://www.newsbtc.com/feed/",
    "Bloomberg_Crypto": "https://feeds.bloomberg.com/crypto/news.rss",
    "Glassnode_Insights": "https://glassnode.substack.com/feed",
}

TIME_WINDOW_START_UTC = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=24)
OUTPUT_FILE = "crypto_news_24h_zhipu_summary_v3.csv"

# â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šä¿®å¤ 429 Too Many Requests é”™è¯¯ â˜…â˜…â˜…
# å¿…é¡»è®¾ç½®ä¸º 1ï¼Œä»¥â€œä¸²è¡Œâ€æ–¹å¼ç¤¼è²Œåœ°è®¿é—®ï¼Œé˜²æ­¢è¢«æœåŠ¡å™¨å±è”½ã€‚
MAX_WORKERS = 1  


# ====== â˜… æŠ“å–æ–‡æœ¬å‡½æ•° (V2 å‡çº§ç‰ˆ - ä½¿ç”¨ newspaper3k) ======
def extract_text_from_url(url):
    """
    ä½¿ç”¨ newspaper3k æ™ºèƒ½æå–æ–‡ç« æ­£æ–‡ã€‚
    """
    try:
        # é…ç½® Article å¯¹è±¡ï¼Œå…³é—­SSLéªŒè¯ä»¥å¢åŠ æˆåŠŸç‡
        article = newspaper.Article(url, fetch_images=False, verbose=False)
        article.config.verify_ssl = False
        
        # ä¸‹è½½å’Œè§£æ
        article.download()
        article.parse()
        
        # æå–çº¯å‡€æ–‡æœ¬
        text = article.text
        
        if not text.strip():
            tqdm.write(f"ğŸŸ¡ æ— æ³•æå–å†…å®¹ (newspaper3k æœªæ‰¾åˆ°æ–‡æœ¬): {url}")
            return None
            
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢APIè¶…é™
        return text[:3000] 
        
    except Exception as e:
        tqdm.write(f"âš ï¸ é“¾æ¥æŠ“å–/æå–å¤±è´¥ (newspaper3k)ï¼š{url} - {e}")
        return None


# ====== æ™ºè°±AI æ‘˜è¦å‡½æ•° (æ— éœ€ä¿®æ”¹) ======
def summarize_text_zhipu(text, title):
    if not text or not text.strip():
        return ""
        
    prompt = f"ä½ æ˜¯ä¸€ä¸ªåŠ å¯†è´§å¸æ–°é—»ç¼–è¾‘ã€‚è¯·ç”¨ç®€ä½“ä¸­æ–‡æ€»ç»“ä»¥ä¸‹æ–°é—»ï¼Œé™åˆ¶åœ¨2-3å¥è¯ï¼Œå¿…é¡»åŒ…å«æ ¸å¿ƒäº‹ä»¶ã€äººç‰©å’Œå…³é”®æ•°å­—ã€‚\n\næ ‡é¢˜ï¼š{title}\n\nè‹±æ–‡åŸæ–‡ï¼š{text[:2500]}"
    
    try:
        # è°ƒç”¨ æ™ºè°±AI
        response = client.chat.completions.create(
            model="glm-4-air",  # ä½¿ç”¨ glm-4-air æ¨¡å‹
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            timeout=20.0,  # è®¾ç½®20ç§’è¶…æ—¶
        )
        return response.choices[0].message.content.strip()
            
    except Exception as e:
        tqdm.write(f"âŒ æ™ºè°±AI API è°ƒç”¨å¤±è´¥ï¼š{title} - {e}")
        return ""


# ====== å¹¶å‘å¤„ç†å‡½æ•° (æ— éœ€ä¿®æ”¹) ======
def process_article(entry, source):
    title = entry.title
    
    # --- æ—¥æœŸè¿‡æ»¤ (æ— éœ€ä¿®æ”¹) ---
    try:
        published_time_struct = entry.get("published_parsed")
        if not published_time_struct:
            tqdm.write(f"ğŸŸ¡ æ—¥æœŸç¼ºå¤± (è·³è¿‡): {title}")
            return None
        article_timestamp_utc = calendar.timegm(published_time_struct)
        article_time_utc = datetime.datetime.fromtimestamp(article_timestamp_utc, tz=datetime.timezone.utc)
    except Exception as e:
        tqdm.write(f"ğŸŸ¡ æ—¥æœŸè§£æå¤±è´¥: {entry.get('published', '')} - {e} (è·³è¿‡)")
        return None
    if article_time_utc < TIME_WINDOW_START_UTC:
        return None 
    # --- æ—¥æœŸè¿‡æ»¤ç»“æŸ ---

    link = fix_coindesk_url(entry.link)
    date_str = entry.get("published", "") 
    
    # â˜… è°ƒç”¨æ–°çš„ã€æ›´å¼ºå¤§çš„æŠ“å–å‡½æ•°
    full_text = extract_text_from_url(link)
    
    if full_text:
        # è°ƒç”¨ æ™ºè°±AI
        summary = summarize_text_zhipu(full_text, title)
        
        if summary:
            return {
                "æ ‡é¢˜": title, "æ‘˜è¦": summary, "é“¾æ¥": link,
                "æ—¥æœŸ": date_str, "æ¥æº": source
            }
    return None  # æŠ“å–å¤±è´¥æˆ–æ€»ç»“å¤±è´¥


# ====== ä¸»å‡½æ•° (æ— éœ€ä¿®æ”¹) ======
def fetch_rss_news():
    all_items = []
    tasks_to_process = []
    print(f"\nğŸ• å¼€å§‹æ”¶é›† RSS æºï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â„¹ï¸ ä»…ä¿ç•™ {TIME_WINDOW_START_UTC.strftime('%Y-%m-%d %H:%M:%S')} (UTC) ä¹‹åçš„æ–°é—»")
    print(f"â„¹ï¸ AI æ‘˜è¦æ¨¡å¼ (æ™ºè°±AI) å·²å¯ç”¨ã€‚")
    print(f"â„¹ï¸ å¹¶å‘æ•°è®¾ç½®ä¸º {MAX_WORKERS} (é˜²æ­¢429é”™è¯¯)ã€‚") # â˜… æ–°å¢æ—¥å¿—

    # é˜¶æ®µ 1ï¼šæ”¶é›†
    rss_progress_bar = tqdm(RSS_FEEDS.items(), desc="ğŸ“¡ 1. æ‰«æRSSæº", unit="æº", leave=False)
    for source, url in rss_progress_bar:
        rss_progress_bar.set_description(f"ğŸ“¡ 1. æ‰«æä¸­: {source}")
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                tasks_to_process.append((entry, source))
        except Exception as e:
            print(f"âŒ æŠ“å–RSSæºå¤±è´¥ï¼š{source} - {e}")
            
    print(f"\nâ„¹ï¸ æ”¶é›†åˆ° {len(tasks_to_process)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹å¹¶å‘å¤„ç†...")

    # é˜¶æ®µ 2ï¼šå¹¶å‘å¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_article, entry, source) for entry, source in tasks_to_process]
        
        ai_progress_bar = tqdm(
            concurrent.futures.as_completed(futures),
            total=len(tasks_to_process),
            desc="ğŸ¤– 2. AI æ‘˜è¦å¤„ç†ä¸­ (æ™ºè°±AI)", 
            unit="ç¯‡"
        )
        
        for future in ai_progress_bar:
            result = future.result()
            if result:
                all_items.append(result)

    # é˜¶æ®µ 3ï¼šä¿å­˜ç»“æœ
    if all_items:
        df = pd.DataFrame(all_items)
        df = df[["æ ‡é¢˜", "æ‘˜è¦", "é“¾æ¥", "æ—¥æœŸ", "æ¥æº"]]
        df.drop_duplicates(subset=["é“¾æ¥"], inplace=True)
        
        try:
            df['parsed_date'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            df = df.sort_values(by='parsed_date', ascending=False).drop(columns=['parsed_date'])
        except Exception:
            print("âš ï¸ æ—¥æœŸæ ¼å¼ä¸ç»Ÿä¸€ï¼Œæœªè¿›è¡Œæ’åºã€‚")
            
        df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å·²ä¿å­˜ {len(df)} æ¡ [AI æ‘˜è¦] æ–°é—»åˆ° {OUTPUT_FILE}")
    else:
        print(f"âš ï¸ æœªè·å–åˆ°è¿‡å»24å°æ—¶å†…çš„æ–°é—»ã€‚")

# (ä¿®å¤ Coindesk é“¾æ¥å‡½æ•°ï¼Œä¿æŒä¸å˜)
def fix_coindesk_url(url: str) -> str:
    if "coindesk.com" not in url: return url
    try:
        fixed = url.replace(",", "/")
        if not fixed.startswith("https://"): fixed = "https://" + fixed.lstrip("/")
        return fixed
    except Exception: return url

# ====== æ‰‹åŠ¨æ‰§è¡Œå…¥å£ ======
if __name__ == "__main__":
    start = time()
    fetch_rss_news()
    print(f"\nâ±ï¸ æ€»è€—æ—¶ï¼š{round(time() - start, 2)} ç§’")
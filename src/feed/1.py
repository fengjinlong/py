import feedparser
import pandas as pd
import datetime
import time
import concurrent.futures
import sys
import calendar
from tqdm import tqdm
import cloudscraper  # â˜… 1. å¯¼å…¥æ–°åº“

# --- AI å’Œç½‘é¡µæŠ“å–åº“å·²å…¨éƒ¨ç§»é™¤ ---

# ====== é…ç½®åŒºåŸŸ ======
RSS_FEEDS = {
    # ok
    # "CoinDesk": "",
    # 'c1':'https://www.coindesk.com/arc/outboundfeeds/rss',
    # 'c2':'https://cointelegraph.com/rss/category/op-ed',
    # 'c3':'https://cointelegraph.com/rss/category/hodlers-digest',
    # 'c4':'https://cointelegraph.com/rss/category/markets',
    # 'c5':'https://cointelegraph.com/rss',
    # 'c6':'https://thedefiant.io/feed/',
    # 'c7':'https://cryptonews.com/news/feed',
    # 'c9':'https://coinjournal.net/news/feed',
    # "Decrypt": "https://decrypt.co/feed",
    # "CryptoSlate": "https://cryptoslate.com/feed/",
    "NewsBTC": "https://rss.app/feeds/uVF07JHLgdFpr61J.xml",
    # "Bloomberg_Crypto": "https://feeds.bloomberg.com/crypto/news.rss",
    # "Reuters_Finance": "https://www.reuters.com/markets/finance/rss/",
    # "Glassnode_Insights": "https://glassnode.substack.com/feed",
    # "CryptoQuant_Blog": "https://cryptoquant.com/feed",
    # "Dune_Blog": "https://dune.com/blog/rss.xml",
    # "Messari_All": "https://messari.io/rss/all.xml",
    # "Delphi_Digital": "https://members.delphidigital.io/feed",
    # "a16z_Crypto_Blog": "https://a16zcrypto.com/feed/",
    # "Paradigm_Blog": "https://www.paradigm.xyz/rss",
    # "Coindesk_Korea": "https://www.coindeskkorea.com/rss/allArticle.xml"
}

# æµè§ˆå™¨ä¼ªè£…å¤´ (ä»ç„¶éœ€è¦)
BROWSER_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
}

TIME_WINDOW_START_UTC = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(hours=24)
OUTPUT_FILE = "crypto_news_TEST.csv" 
MAX_WORKERS = 20


# ====== ä¿®å¤ Coindesk é“¾æ¥ (ä¿ç•™) ======
def fix_coindesk_url(url: str) -> str:
    if "coindesk.com" not in url: return url
    try:
        fixed = url.replace(",", "/")
        if not fixed.startswith("https://"): fixed = "https://" + fixed.lstrip("/")
        return fixed
    except Exception: return url

# ====== å¹¶å‘å¤„ç†å‡½æ•° (ä¸å˜) ======
def process_article(entry, source):
    """
    åªè¿›è¡Œæ—¥æœŸè¿‡æ»¤ï¼Œä¸æŠ“å–ç½‘é¡µï¼Œä¸è°ƒç”¨AI
    """
    title = entry.get('title', 'No Title Provided')
    
    # --- æ—¥æœŸè¿‡æ»¤ (ä¿ç•™) ---
    article_time_utc = None
    try:
        published_time_struct = entry.get("published_parsed")
        if not published_time_struct:
            tqdm.write(f"ğŸŸ¡ æ—¥æœŸç¼ºå¤± (è·³è¿‡): {title}")
            return None
        article_timestamp_utc = calendar.timegm(published_time_struct)
        article_time_utc = datetime.datetime.fromtimestamp(article_timestamp_utc, tz=datetime.timezone.utc)
    except Exception as e:
        tqdm.write(f"ğŸŸ¡ æ—¥æœŸè§£æå¤±è´¥: {entry.get('published', '')} - {e} (è·³è¿‡)")
        return None
    if article_time_utc < TIME_WINDOW_START_UTC:
        return None 
    # --- æ—¥æœŸè¿‡æ»¤ç»“æŸ ---

    link = fix_coindesk_url(entry.link)
    date_str = entry.get("published", "") # è¿™æ˜¯ Published å­—æ®µ
    
    return {
        "Title": title,
        "Summary": "", 
        "Link": link,
        "Published": date_str,
        "Source": source,
        "Date Batch": article_time_utc.strftime("%Yå¹´%mæœˆ%dæ—¥"), 
        "Description": "", 
    }


# ====== â˜… æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šä¸»å‡½æ•°ï¼ˆä½¿ç”¨ cloudscraperï¼‰ ======
def fetch_rss_news():
    all_items = []
    tasks_to_process = []
    print(f"\nğŸ• å¼€å§‹æ”¶é›† RSS æºï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â„¹ï¸ ä»…ä¿ç•™ {TIME_WINDOW_START_UTC.strftime('%Y-%m-%d %H:%M:%S')} (UTC) ä¹‹åçš„æ–°é—»")
    print("â„¹ï¸ AI æ‘˜è¦æ¨¡å¼ (å·²å…³é—­)ã€‚")

    # â˜… 2. åˆ›å»ºä¸€ä¸ª scraper å®ä¾‹
    # (browser='chrome' å¯ä»¥æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨æŒ‡çº¹)
    scraper = cloudscraper.create_scraper(browser='chrome') 

    # é˜¶æ®µ 1ï¼šæ”¶é›† (â˜… ç›®æ ‡1: éªŒè¯RSSæº)
    rss_progress_bar = tqdm(RSS_FEEDS.items(), desc="ğŸ“¡ 1. æ‰«æRSSæº", unit="æº", leave=False)
    for source, url in rss_progress_bar:
        rss_progress_bar.set_description(f"ğŸ“¡ 1. æ‰«æä¸­: {source}")
        try:
            # â˜… 3. ä½¿ç”¨ scraper.get() æ¥ä¸‹è½½å†…å®¹
            response = scraper.get(url, headers=BROWSER_HEADERS, timeout=15)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code != 200:
                print(f"âŒ æŠ“å–å¤±è´¥ (HTTP {response.status_code})ï¼š{source}")
                continue # è·³è¿‡è¿™ä¸ªæº

            # â˜… 4. æŠŠä¸‹è½½å¥½çš„æ–‡æœ¬ (response.text) å–‚ç»™ feedparser
            feed = feedparser.parse(response.text) 
            
            if feed.bozo:
                # è¿™é‡Œçš„ bozo é”™è¯¯ç°åœ¨æ›´å¯èƒ½æ˜¯çœŸå®çš„XMLæ ¼å¼é—®é¢˜
                print(f"âš ï¸ è§£æè­¦å‘Šï¼š{source} (RSSæ ¼å¼å¯èƒ½ä¸è§„èŒƒ) - {feed.bozo_exception}")
                
            if not feed.entries:
                 tqdm.write(f"ğŸŸ¡ æºå†…å®¹ä¸ºç©ºï¼š{source} (å¯èƒ½æŠ“å–è¢«æ‹¦æˆªæˆ–è¯¥æºæ— æ–°é—»)")
            
            for entry in feed.entries:
                tasks_to_process.append((entry, source))
                
        except Exception as e:
            # è¿™é‡Œçš„æ—¥å¿—ä¼šæ•è·è¶…æ—¶ã€è¿æ¥é”™è¯¯ç­‰
            print(f"âŒ æŠ“å–RSSæºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{source} - {e}")
            
    print(f"\nâ„¹ï¸ æ”¶é›†åˆ° {len(tasks_to_process)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹å¹¶å‘å¤„ç†...")

    # é˜¶æ®µ 2ï¼šå¹¶å‘å¤„ç† (ä¸å˜)
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_article, entry, source) for entry, source in tasks_to_process]
        
        ai_progress_bar = tqdm(
            concurrent.futures.as_completed(futures),
            total=len(tasks_to_process),
            desc="ğŸ“° 2. RSS æ¡ç›®å¤„ç†ä¸­", 
            unit="ç¯‡"
        )
        
        for future in ai_progress_bar:
            result = future.result()
            if result:
                all_items.append(result)

    # é˜¶æ®µ 3ï¼šä¿å­˜ç»“æœ (ä¸å˜)
    if all_items:
        df = pd.DataFrame(all_items)
        
        final_columns = [
            "Title", "Date Batch", "Description", "Link", 
            "Published", "Source", "Summary"
        ]
        present_columns = [col for col in final_columns if col in df.columns]
        df = df[present_columns] 
        
        df.drop_duplicates(subset=["Link"], inplace=True)
        
        try:
            df['parsed_date'] = pd.to_datetime(df['Published'], errors='coerce')
            df = df.sort_values(by='parsed_date', ascending=False).drop(columns=['parsed_date'])
        except Exception:
            print("âš ï¸ æ—¥æœŸæ ¼å¼ä¸ç»Ÿä¸€ï¼Œæœªè¿›è¡Œæ’åºã€‚")
            
        df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å·²ä¿å­˜ {len(df)} æ¡æ–°é—»æ¡ç›®åˆ° {OUTPUT_FILE}")
    else:
        print(f"âš ï¸ æœªè·å–åˆ°è¿‡å»24å°æ—¶å†…çš„æ–°é—»ã€‚")


# ====== æ‰‹åŠ¨æ‰§è¡Œå…¥å£ ======
if __name__ == "__main__":
    start = time.time()
    fetch_rss_news()
    print(f"\nâ±ï¸ æ€»è€—æ—¶ï¼š{round(time.time() - start, 2)} ç§’")